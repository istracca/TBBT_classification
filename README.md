# Evaluating Parameter-Efficient Fine-Tuning for Character Classification in Dialogue Data: A Case Study on *The Big Bang Theory*

This project investigates the effectiveness of parameter-efficient fine-tuning strategies for speaker classification in dialogue data. We use dialogue excerpts from the TV series *The Big Bang Theory* and compare different fine-tuning techniques on sentence embeddings generated by the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) SBERT model.

---

## 🧠 Task Overview

We frame the problem as a multi-class or binary classification task depending on the setting. The goal is to classify the speaker of a given line of dialogue.

### Classification Tasks
- **All-vs-All**: Classify among the 7 main characters.
- **Sheldon vs Penny**
- **Sheldon vs Leonard**

### Loss Functions
- Standard **Cross Entropy**
- **Weighted Cross Entropy** to mitigate class imbalance

### Model Variants
All models share the same classifier head: a single hidden-layer MLP with 128 neurons. We compare three fine-tuning strategies:
- **Frozen SBERT + MLP**: Only the MLP is trained.
- **LoRA**: Low-Rank Adaptation applied to SBERT; same MLP head.
- **Last-Layer Unfreeze**: All SBERT parameters frozen except the last layer; MLP head is trained jointly.

---

## 🎯 Bonus: Character Trajectories over Time

As an additional exploratory task, we analyze how character representations evolve across the 10 seasons:
- Compute **average embeddings** per character per season using the best fine-tuned model (Last-Layer + Weighted Loss).
- Visualize the evolution with **t-SNE** to provide a qualitative view of character development.

---

## 📁 Project Structure

```
TBBT_classification/
├── data/
│   ├── raw/
│   └── processed/
├── notebooks/
│   ├── 01_create_dataset.ipynb
│   ├── 02_grid_search_classifier_only.ipynb
│   ├── 03_grid_search_lora.ipynb
│   ├── 04_grid_search_last_layer.ipynb
│   ├── 05_evaluate_best_models.ipynb
│   ├── 06_create_latex_tables.ipynb
│   ├── 07_compute_finetuned_embeddings.ipynb
│   └── 08_tsne_characters_evolution.ipynb
├── src/
│   ├── my_classes.py
│   ├── utils.py
│   └── utils_latex.py
├── models/
│   ├── classifier_only/
│   ├── lora/
│   └── last_layer/
├── metrics/
│   ├── classifier_only/
│   ├── lora/
│   └── last_layer/
├── images/
├── README.md
└── requirements.txt
```



---

## 🚀 Getting Started

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Prepare the dataset
Make sure the raw data file is available at:

```bash
data/raw/1_10_seasons_tbbt.csv
```

Precomputed embeddings are saved under:

```bash
data/processed/
```

### 3. Run the notebooks
You can follow the analysis step-by-step through the notebooks:

01_create_dataset.ipynb: Preprocessing and dataset creation

02–04: Grid search for each model type

05: Evaluation of best models

06: Generate LaTeX tables for the paper/report

07–08: Compute and visualize fine-tuned embeddings with t-SNE

## 📌 Dependencies
Key Python libraries (see requirements.txt for full list):

transformers

sentence-transformers

scikit-learn

pandas, numpy

matplotlib, seaborn

peft, accelerate (for LoRA fine-tuning)

## 👤 Author
Iacopo Stracca
Project developed as part of a university course assignment. No license applies.
