# Evaluating Parameter-Efficient Fine-Tuning for Character Classification in Dialogue Data: A Case Study on *The Big Bang Theory*

This project investigates the effectiveness of parameter-efficient fine-tuning strategies for speaker classification in dialogue data. We use dialogue excerpts from the TV series *The Big Bang Theory* and compare different fine-tuning techniques on sentence embeddings generated by the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) SBERT model.

---

## ğŸ§  Task Overview

We frame the problem as a multi-class or binary classification task depending on the setting. The goal is to classify the speaker of a given line of dialogue.

### Classification Tasks
- **All-vs-All**: Classify among the 7 main characters.
- **Sheldon vs Penny**
- **Sheldon vs Leonard**

### Loss Functions
- Standard **Cross Entropy**
- **Weighted Cross Entropy** to mitigate class imbalance

### Model Variants
All models share the same classifier head: a single hidden-layer MLP with 128 neurons. We compare three fine-tuning strategies:
- **Frozen SBERT + MLP**: Only the MLP is trained.
- **LoRA**: Low-Rank Adaptation applied to SBERT; same MLP head.
- **Last-Layer Unfreeze**: All SBERT parameters frozen except the last layer; MLP head is trained jointly.

---

## ğŸ¯ Bonus: Character Trajectories over Time

As an additional exploratory task, we analyze how character representations evolve across the 10 seasons:
- Compute **average embeddings** per character per season using the best fine-tuned model (Last-Layer + Weighted Loss).
- Visualize the evolution with **t-SNE** to provide a qualitative view of character development.

---

## ğŸ“ Project Structure

```
TBBT_classification/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_create_dataset.ipynb
â”‚   â”œâ”€â”€ 02_grid_search_classifier_only.ipynb
â”‚   â”œâ”€â”€ 03_grid_search_lora.ipynb
â”‚   â”œâ”€â”€ 04_grid_search_last_layer.ipynb
â”‚   â”œâ”€â”€ 05_evaluate_best_models.ipynb
â”‚   â”œâ”€â”€ 06_create_latex_tables.ipynb
â”‚   â”œâ”€â”€ 07_compute_finetuned_embeddings.ipynb
â”‚   â””â”€â”€ 08_tsne_characters_evolution.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ my_classes.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ utils_latex.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ classifier_only/
â”‚   â”œâ”€â”€ lora/
â”‚   â””â”€â”€ last_layer/
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ classifier_only/
â”‚   â”œâ”€â”€ lora/
â”‚   â””â”€â”€ last_layer/
â”œâ”€â”€ images/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```



---

## ğŸš€ Getting Started

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Prepare the dataset
Make sure the raw data file is available at:

```bash
data/raw/1_10_seasons_tbbt.csv
```

Precomputed embeddings are saved under:

```bash
data/processed/
```

### 3. Run the notebooks
You can follow the analysis step-by-step through the notebooks:

01_create_dataset.ipynb: Preprocessing and dataset creation

02â€“04: Grid search for each model type

05: Evaluation of best models

06: Generate LaTeX tables for the paper/report

07â€“08: Compute and visualize fine-tuned embeddings with t-SNE

## ğŸ“Œ Dependencies
Key Python libraries (see requirements.txt for full list):

transformers

sentence-transformers

scikit-learn

pandas, numpy

matplotlib, seaborn

peft, accelerate (for LoRA fine-tuning)

## ğŸ‘¤ Author
Iacopo Stracca
Project developed as part of a university course assignment. No license applies.
