{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe81eae6",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7aa0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Adding 'src' directory to the system path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "from utils_latex import print_results_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840d185",
   "metadata": {},
   "source": [
    "Print latex tables for Classifier_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fda6f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all with cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "learning_rate & dropout_rate & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "5e-04 & 0.1000 & 5e-04 & 98 & 1.5351 & 1.6786 & 0.4291 & 0.3492 \\\\\n",
      "1e-04 & 0.2000 & 5e-04 & 245 & 1.5756 & 1.6860 & 0.4108 & 0.3498 \\\\\n",
      "1e-04 & 0.1000 & 5e-04 & 231 & 1.5708 & 1.6869 & 0.4123 & 0.3503 \\\\\n",
      "5e-04 & 0.2000 & 5e-04 & 62 & 1.5860 & 1.6871 & 0.4052 & 0.3522 \\\\\n",
      "5e-05 & 0.1000 & 5e-04 & 452 & 1.5720 & 1.6878 & 0.4119 & 0.3449 \\\\\n",
      "1e-04 & 0.2000 & 1e-04 & 113 & 1.5437 & 1.6884 & 0.4204 & 0.3416 \\\\\n",
      "5e-04 & 0.0000 & 5e-04 & 53 & 1.5642 & 1.6907 & 0.4162 & 0.3414 \\\\\n",
      "5e-04 & 0.2000 & 1e-04 & 31 & 1.5134 & 1.6913 & 0.4311 & 0.3487 \\\\\n",
      "1e-04 & 0.1000 & 1e-04 & 110 & 1.5350 & 1.6918 & 0.4242 & 0.3467 \\\\\n",
      "1e-04 & 0.0000 & 5e-04 & 182 & 1.5848 & 1.6918 & 0.4061 & 0.3478 \\\\\n",
      "5e-05 & 0.2000 & 5e-04 & 249 & 1.6178 & 1.6924 & 0.3883 & 0.3421 \\\\\n",
      "5e-05 & 0.0000 & 5e-04 & 344 & 1.5953 & 1.6924 & 0.4018 & 0.3434 \\\\\n",
      "5e-05 & 0.2000 & 1e-04 & 196 & 1.5646 & 1.6928 & 0.4109 & 0.3489 \\\\\n",
      "1e-05 & 0.1000 & 1e-04 & 883 & 1.5730 & 1.6930 & 0.4085 & 0.3491 \\\\\n",
      "5e-05 & 0.2000 & 0e+00 & 159 & 1.5695 & 1.6930 & 0.4078 & 0.3496 \\\\\n",
      "1e-04 & 0.2000 & 0e+00 & 86 & 1.5618 & 1.6933 & 0.4102 & 0.3492 \\\\\n",
      "1e-05 & 0.2000 & 1e-04 & 899 & 1.5793 & 1.6935 & 0.4042 & 0.3434 \\\\\n",
      "1e-04 & 0.1000 & 0e+00 & 81 & 1.5611 & 1.6938 & 0.4119 & 0.3438 \\\\\n",
      "5e-05 & 0.1000 & 1e-04 & 233 & 1.5310 & 1.6946 & 0.4263 & 0.3418 \\\\\n",
      "5e-05 & 0.0000 & 1e-04 & 192 & 1.5571 & 1.6948 & 0.4157 & 0.3423 \\\\\n",
      "1e-05 & 0.0000 & 1e-04 & 1000 & 1.5610 & 1.6948 & 0.4114 & 0.3461 \\\\\n",
      "1e-05 & 0.2000 & 0e+00 & 605 & 1.6045 & 1.6950 & 0.3907 & 0.3423 \\\\\n",
      "1e-04 & 0.0000 & 1e-04 & 104 & 1.5388 & 1.6951 & 0.4245 & 0.3441 \\\\\n",
      "1e-05 & 0.2000 & 5e-04 & 1000 & 1.6296 & 1.6955 & 0.3835 & 0.3401 \\\\\n",
      "5e-05 & 0.0000 & 0e+00 & 161 & 1.5639 & 1.6960 & 0.4112 & 0.3489 \\\\\n",
      "1e-05 & 0.0000 & 0e+00 & 773 & 1.5699 & 1.6961 & 0.4094 & 0.3465 \\\\\n",
      "5e-04 & 0.1000 & 1e-04 & 25 & 1.5332 & 1.6963 & 0.4227 & 0.3440 \\\\\n",
      "1e-05 & 0.1000 & 5e-04 & 999 & 1.6295 & 1.6963 & 0.3794 & 0.3372 \\\\\n",
      "5e-05 & 0.1000 & 0e+00 & 155 & 1.5689 & 1.6973 & 0.4081 & 0.3423 \\\\\n",
      "5e-04 & 0.0000 & 0e+00 & 17 & 1.5416 & 1.6973 & 0.4210 & 0.3474 \\\\\n",
      "5e-04 & 0.0000 & 1e-04 & 22 & 1.5461 & 1.6984 & 0.4214 & 0.3429 \\\\\n",
      "1e-05 & 0.1000 & 0e+00 & 586 & 1.6009 & 1.6991 & 0.3927 & 0.3412 \\\\\n",
      "1e-04 & 0.0000 & 0e+00 & 88 & 1.5483 & 1.6991 & 0.4166 & 0.3474 \\\\\n",
      "1e-05 & 0.0000 & 5e-04 & 997 & 1.6351 & 1.7001 & 0.3773 & 0.3361 \\\\\n",
      "5e-04 & 0.0000 & 1.0e-03 & 88 & 1.6583 & 1.7006 & 0.3703 & 0.3408 \\\\\n",
      "5e-04 & 0.1000 & 1.0e-03 & 72 & 1.6655 & 1.7022 & 0.3668 & 0.3414 \\\\\n",
      "1e-04 & 0.0000 & 1.0e-03 & 247 & 1.6557 & 1.7024 & 0.3713 & 0.3374 \\\\\n",
      "5e-04 & 0.1000 & 0e+00 & 20 & 1.5218 & 1.7024 & 0.4318 & 0.3382 \\\\\n",
      "5e-04 & 0.2000 & 1.0e-03 & 63 & 1.6742 & 1.7036 & 0.3613 & 0.3356 \\\\\n",
      "5e-05 & 0.2000 & 1.0e-03 & 329 & 1.6619 & 1.7037 & 0.3690 & 0.3400 \\\\\n",
      "5e-04 & 0.2000 & 0e+00 & 26 & 1.4984 & 1.7037 & 0.4385 & 0.3459 \\\\\n",
      "1e-04 & 0.1000 & 1.0e-03 & 134 & 1.6632 & 1.7040 & 0.3669 & 0.3380 \\\\\n",
      "1e-04 & 0.2000 & 1.0e-03 & 117 & 1.6689 & 1.7057 & 0.3626 & 0.3358 \\\\\n",
      "5e-05 & 0.0000 & 1.0e-03 & 293 & 1.6662 & 1.7065 & 0.3633 & 0.3338 \\\\\n",
      "5e-05 & 0.1000 & 1.0e-03 & 188 & 1.6695 & 1.7071 & 0.3606 & 0.3338 \\\\\n",
      "1e-05 & 0.1000 & 1.0e-03 & 1000 & 1.6682 & 1.7071 & 0.3634 & 0.3351 \\\\\n",
      "1e-05 & 0.2000 & 1.0e-03 & 1000 & 1.6699 & 1.7076 & 0.3633 & 0.3345 \\\\\n",
      "1e-05 & 0.0000 & 1.0e-03 & 1000 & 1.6694 & 1.7086 & 0.3623 & 0.3323 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for all with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "learning_rate & dropout_rate & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "5e-04 & 0.2000 & 5e-04 & 47 & 1.6729 & 1.8102 & 0.3555 & 0.2972 \\\\\n",
      "5e-04 & 0.1000 & 5e-04 & 47 & 1.6487 & 1.8136 & 0.3640 & 0.2906 \\\\\n",
      "5e-04 & 0.2000 & 1e-04 & 20 & 1.6698 & 1.8156 & 0.3561 & 0.2829 \\\\\n",
      "5e-04 & 0.0000 & 5e-04 & 38 & 1.6779 & 1.8172 & 0.3561 & 0.2829 \\\\\n",
      "1e-04 & 0.2000 & 5e-04 & 112 & 1.7217 & 1.8174 & 0.3344 & 0.2877 \\\\\n",
      "5e-05 & 0.2000 & 5e-04 & 211 & 1.7231 & 1.8175 & 0.3362 & 0.2860 \\\\\n",
      "1e-04 & 0.1000 & 5e-04 & 164 & 1.6866 & 1.8179 & 0.3513 & 0.2875 \\\\\n",
      "5e-04 & 0.1000 & 1.0e-03 & 80 & 1.7673 & 1.8186 & 0.3152 & 0.2918 \\\\\n",
      "1e-04 & 0.0000 & 5e-04 & 123 & 1.7144 & 1.8190 & 0.3397 & 0.2871 \\\\\n",
      "5e-05 & 0.1000 & 5e-04 & 264 & 1.7086 & 1.8194 & 0.3420 & 0.2858 \\\\\n",
      "1e-04 & 0.2000 & 1e-04 & 55 & 1.7299 & 1.8197 & 0.3311 & 0.2807 \\\\\n",
      "5e-04 & 0.2000 & 1.0e-03 & 54 & 1.7777 & 1.8203 & 0.3134 & 0.2829 \\\\\n",
      "1e-04 & 0.2000 & 0e+00 & 49 & 1.7284 & 1.8205 & 0.3304 & 0.2842 \\\\\n",
      "5e-05 & 0.2000 & 0e+00 & 115 & 1.7105 & 1.8210 & 0.3375 & 0.2895 \\\\\n",
      "5e-05 & 0.2000 & 1e-04 & 134 & 1.7090 & 1.8212 & 0.3410 & 0.2833 \\\\\n",
      "1e-05 & 0.2000 & 0e+00 & 462 & 1.7363 & 1.8213 & 0.3258 & 0.2838 \\\\\n",
      "5e-04 & 0.0000 & 1.0e-03 & 63 & 1.7680 & 1.8215 & 0.3155 & 0.2820 \\\\\n",
      "5e-05 & 0.0000 & 5e-04 & 249 & 1.7174 & 1.8215 & 0.3392 & 0.2886 \\\\\n",
      "5e-05 & 0.1000 & 0e+00 & 133 & 1.6963 & 1.8218 & 0.3444 & 0.2873 \\\\\n",
      "1e-04 & 0.1000 & 1e-04 & 65 & 1.7035 & 1.8221 & 0.3433 & 0.2807 \\\\\n",
      "5e-05 & 0.1000 & 1e-04 & 139 & 1.7023 & 1.8222 & 0.3406 & 0.2862 \\\\\n",
      "1e-05 & 0.2000 & 5e-04 & 736 & 1.7499 & 1.8224 & 0.3217 & 0.2818 \\\\\n",
      "5e-05 & 0.0000 & 1e-04 & 140 & 1.7067 & 1.8228 & 0.3411 & 0.2856 \\\\\n",
      "5e-04 & 0.1000 & 1e-04 & 24 & 1.6133 & 1.8229 & 0.3788 & 0.2951 \\\\\n",
      "1e-04 & 0.1000 & 0e+00 & 56 & 1.7079 & 1.8229 & 0.3378 & 0.2864 \\\\\n",
      "5e-04 & 0.0000 & 1e-04 & 18 & 1.6743 & 1.8230 & 0.3541 & 0.2860 \\\\\n",
      "1e-04 & 0.2000 & 1.0e-03 & 157 & 1.7682 & 1.8231 & 0.3166 & 0.2862 \\\\\n",
      "5e-05 & 0.2000 & 1.0e-03 & 267 & 1.7691 & 1.8233 & 0.3159 & 0.2833 \\\\\n",
      "1e-04 & 0.0000 & 1e-04 & 83 & 1.6770 & 1.8234 & 0.3548 & 0.2820 \\\\\n",
      "1e-04 & 0.1000 & 1.0e-03 & 150 & 1.7710 & 1.8238 & 0.3155 & 0.2851 \\\\\n",
      "1e-05 & 0.2000 & 1e-04 & 364 & 1.7595 & 1.8244 & 0.3209 & 0.2854 \\\\\n",
      "5e-04 & 0.1000 & 0e+00 & 17 & 1.6468 & 1.8254 & 0.3656 & 0.2922 \\\\\n",
      "1e-05 & 0.1000 & 1e-04 & 412 & 1.7499 & 1.8259 & 0.3225 & 0.2844 \\\\\n",
      "1e-05 & 0.1000 & 5e-04 & 526 & 1.7635 & 1.8261 & 0.3189 & 0.2831 \\\\\n",
      "1e-04 & 0.0000 & 0e+00 & 62 & 1.7075 & 1.8261 & 0.3409 & 0.2831 \\\\\n",
      "5e-04 & 0.2000 & 0e+00 & 17 & 1.6653 & 1.8263 & 0.3549 & 0.2840 \\\\\n",
      "1e-05 & 0.1000 & 0e+00 & 342 & 1.7522 & 1.8270 & 0.3206 & 0.2814 \\\\\n",
      "5e-05 & 0.1000 & 1.0e-03 & 147 & 1.7823 & 1.8278 & 0.3111 & 0.2822 \\\\\n",
      "5e-04 & 0.0000 & 0e+00 & 16 & 1.6529 & 1.8284 & 0.3621 & 0.2902 \\\\\n",
      "1e-05 & 0.0000 & 0e+00 & 240 & 1.7692 & 1.8284 & 0.3162 & 0.2853 \\\\\n",
      "1e-05 & 0.0000 & 5e-04 & 304 & 1.7785 & 1.8286 & 0.3137 & 0.2831 \\\\\n",
      "5e-05 & 0.0000 & 1.0e-03 & 140 & 1.7835 & 1.8286 & 0.3104 & 0.2811 \\\\\n",
      "1e-05 & 0.2000 & 1.0e-03 & 535 & 1.7873 & 1.8288 & 0.3092 & 0.2818 \\\\\n",
      "1e-05 & 0.0000 & 1e-04 & 241 & 1.7730 & 1.8291 & 0.3132 & 0.2833 \\\\\n",
      "1e-05 & 0.1000 & 1.0e-03 & 543 & 1.7854 & 1.8293 & 0.3114 & 0.2851 \\\\\n",
      "1e-04 & 0.0000 & 1.0e-03 & 68 & 1.7873 & 1.8293 & 0.3087 & 0.2813 \\\\\n",
      "5e-05 & 0.0000 & 0e+00 & 54 & 1.7669 & 1.8298 & 0.3172 & 0.2840 \\\\\n",
      "1e-05 & 0.0000 & 1.0e-03 & 461 & 1.7866 & 1.8299 & 0.3111 & 0.2838 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Penny with cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "learning_rate & dropout_rate & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "1e-04 & 0.2000 & 1.0e-03 & 247 & 0.4101 & 0.4736 & 0.8214 & 0.7604 \\\\\n",
      "5e-05 & 0.1000 & 1e-04 & 216 & 0.3613 & 0.4751 & 0.8504 & 0.7568 \\\\\n",
      "1e-05 & 0.1000 & 1e-04 & 997 & 0.3909 & 0.4752 & 0.8277 & 0.7568 \\\\\n",
      "1e-05 & 0.2000 & 0e+00 & 824 & 0.4053 & 0.4765 & 0.8186 & 0.7573 \\\\\n",
      "1e-04 & 0.2000 & 1e-04 & 107 & 0.3743 & 0.4766 & 0.8364 & 0.7609 \\\\\n",
      "1e-04 & 0.1000 & 5e-04 & 124 & 0.4046 & 0.4767 & 0.8200 & 0.7615 \\\\\n",
      "5e-05 & 0.1000 & 0e+00 & 180 & 0.3702 & 0.4769 & 0.8416 & 0.7630 \\\\\n",
      "1e-04 & 0.2000 & 5e-04 & 152 & 0.3844 & 0.4771 & 0.8338 & 0.7583 \\\\\n",
      "1e-04 & 0.1000 & 1e-04 & 92 & 0.3944 & 0.4771 & 0.8251 & 0.7516 \\\\\n",
      "5e-05 & 0.2000 & 1e-04 & 194 & 0.3836 & 0.4773 & 0.8330 & 0.7630 \\\\\n",
      "1e-05 & 0.1000 & 0e+00 & 796 & 0.4149 & 0.4774 & 0.8127 & 0.7578 \\\\\n",
      "5e-05 & 0.1000 & 5e-04 & 265 & 0.4039 & 0.4780 & 0.8212 & 0.7625 \\\\\n",
      "5e-05 & 0.2000 & 5e-04 & 254 & 0.4109 & 0.4780 & 0.8168 & 0.7589 \\\\\n",
      "5e-05 & 0.2000 & 0e+00 & 181 & 0.3930 & 0.4782 & 0.8231 & 0.7562 \\\\\n",
      "5e-05 & 0.0000 & 5e-04 & 251 & 0.4265 & 0.4786 & 0.8060 & 0.7562 \\\\\n",
      "1e-05 & 0.2000 & 1e-04 & 838 & 0.4174 & 0.4787 & 0.8078 & 0.7562 \\\\\n",
      "1e-04 & 0.1000 & 0e+00 & 93 & 0.3724 & 0.4788 & 0.8401 & 0.7562 \\\\\n",
      "1e-04 & 0.2000 & 0e+00 & 87 & 0.3894 & 0.4789 & 0.8306 & 0.7651 \\\\\n",
      "1e-04 & 0.0000 & 1e-04 & 96 & 0.3808 & 0.4790 & 0.8360 & 0.7589 \\\\\n",
      "1e-05 & 0.0000 & 0e+00 & 762 & 0.4072 & 0.4795 & 0.8163 & 0.7583 \\\\\n",
      "1e-05 & 0.2000 & 5e-04 & 999 & 0.4367 & 0.4796 & 0.7957 & 0.7557 \\\\\n",
      "5e-04 & 0.1000 & 1.0e-03 & 49 & 0.4231 & 0.4797 & 0.8102 & 0.7594 \\\\\n",
      "1e-05 & 0.1000 & 5e-04 & 999 & 0.4382 & 0.4797 & 0.7963 & 0.7557 \\\\\n",
      "5e-05 & 0.0000 & 0e+00 & 158 & 0.3984 & 0.4800 & 0.8229 & 0.7583 \\\\\n",
      "5e-04 & 0.2000 & 5e-04 & 45 & 0.3438 & 0.4801 & 0.8583 & 0.7552 \\\\\n",
      "1e-05 & 0.0000 & 1e-04 & 846 & 0.4003 & 0.4803 & 0.8253 & 0.7604 \\\\\n",
      "1e-04 & 0.1000 & 1.0e-03 & 188 & 0.4373 & 0.4804 & 0.8020 & 0.7557 \\\\\n",
      "1e-05 & 0.0000 & 5e-04 & 999 & 0.4376 & 0.4806 & 0.7992 & 0.7562 \\\\\n",
      "1e-04 & 0.0000 & 1.0e-03 & 233 & 0.4287 & 0.4808 & 0.8059 & 0.7604 \\\\\n",
      "5e-04 & 0.0000 & 1.0e-03 & 87 & 0.3819 & 0.4808 & 0.8403 & 0.7547 \\\\\n",
      "5e-05 & 0.0000 & 1e-04 & 167 & 0.4048 & 0.4808 & 0.8209 & 0.7583 \\\\\n",
      "1e-04 & 0.0000 & 0e+00 & 85 & 0.3848 & 0.4810 & 0.8311 & 0.7552 \\\\\n",
      "1e-04 & 0.0000 & 5e-04 & 156 & 0.3829 & 0.4813 & 0.8353 & 0.7536 \\\\\n",
      "5e-04 & 0.1000 & 5e-04 & 36 & 0.3594 & 0.4815 & 0.8487 & 0.7578 \\\\\n",
      "5e-04 & 0.1000 & 1e-04 & 25 & 0.3468 & 0.4827 & 0.8539 & 0.7500 \\\\\n",
      "5e-04 & 0.2000 & 1.0e-03 & 41 & 0.4323 & 0.4832 & 0.8017 & 0.7615 \\\\\n",
      "5e-04 & 0.1000 & 0e+00 & 21 & 0.3541 & 0.4837 & 0.8488 & 0.7500 \\\\\n",
      "5e-04 & 0.2000 & 1e-04 & 27 & 0.3451 & 0.4837 & 0.8556 & 0.7568 \\\\\n",
      "5e-05 & 0.2000 & 1.0e-03 & 288 & 0.4554 & 0.4838 & 0.7854 & 0.7536 \\\\\n",
      "5e-04 & 0.0000 & 5e-04 & 35 & 0.3768 & 0.4845 & 0.8389 & 0.7573 \\\\\n",
      "5e-04 & 0.0000 & 1e-04 & 18 & 0.3955 & 0.4848 & 0.8250 & 0.7620 \\\\\n",
      "5e-05 & 0.0000 & 1.0e-03 & 343 & 0.4571 & 0.4864 & 0.7876 & 0.7526 \\\\\n",
      "1e-05 & 0.0000 & 1.0e-03 & 998 & 0.4735 & 0.4878 & 0.7753 & 0.7536 \\\\\n",
      "5e-04 & 0.2000 & 0e+00 & 22 & 0.3584 & 0.4878 & 0.8474 & 0.7589 \\\\\n",
      "5e-05 & 0.1000 & 1.0e-03 & 241 & 0.4732 & 0.4883 & 0.7760 & 0.7542 \\\\\n",
      "5e-04 & 0.0000 & 0e+00 & 20 & 0.3534 & 0.4886 & 0.8484 & 0.7536 \\\\\n",
      "1e-05 & 0.2000 & 1.0e-03 & 998 & 0.4797 & 0.4887 & 0.7704 & 0.7583 \\\\\n",
      "1e-05 & 0.1000 & 1.0e-03 & 496 & 0.5004 & 0.4948 & 0.7542 & 0.7526 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Penny with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "learning_rate & dropout_rate & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "1e-04 & 0.1000 & 5e-04 & 152 & 0.3906 & 0.4867 & 0.8323 & 0.7516 \\\\\n",
      "1e-04 & 0.2000 & 0e+00 & 95 & 0.3881 & 0.4869 & 0.8306 & 0.7432 \\\\\n",
      "5e-05 & 0.1000 & 5e-04 & 269 & 0.4065 & 0.4874 & 0.8242 & 0.7604 \\\\\n",
      "5e-05 & 0.2000 & 5e-04 & 275 & 0.4019 & 0.4875 & 0.8267 & 0.7490 \\\\\n",
      "1e-04 & 0.0000 & 5e-04 & 153 & 0.3874 & 0.4878 & 0.8336 & 0.7484 \\\\\n",
      "1e-05 & 0.1000 & 1e-04 & 890 & 0.4135 & 0.4885 & 0.8161 & 0.7526 \\\\\n",
      "1e-04 & 0.0000 & 0e+00 & 86 & 0.3922 & 0.4895 & 0.8312 & 0.7500 \\\\\n",
      "5e-05 & 0.2000 & 1.0e-03 & 404 & 0.4296 & 0.4899 & 0.8067 & 0.7438 \\\\\n",
      "5e-05 & 0.2000 & 0e+00 & 186 & 0.3941 & 0.4900 & 0.8278 & 0.7458 \\\\\n",
      "1e-04 & 0.2000 & 5e-04 & 122 & 0.4164 & 0.4901 & 0.8140 & 0.7484 \\\\\n",
      "1e-05 & 0.1000 & 0e+00 & 756 & 0.4265 & 0.4901 & 0.8061 & 0.7474 \\\\\n",
      "1e-04 & 0.1000 & 1e-04 & 95 & 0.3928 & 0.4902 & 0.8272 & 0.7448 \\\\\n",
      "1e-04 & 0.2000 & 1.0e-03 & 195 & 0.4372 & 0.4902 & 0.8010 & 0.7521 \\\\\n",
      "1e-04 & 0.0000 & 1.0e-03 & 214 & 0.4342 & 0.4903 & 0.8045 & 0.7516 \\\\\n",
      "5e-04 & 0.2000 & 1.0e-03 & 58 & 0.4097 & 0.4905 & 0.8209 & 0.7490 \\\\\n",
      "1e-04 & 0.2000 & 1e-04 & 109 & 0.3717 & 0.4908 & 0.8401 & 0.7547 \\\\\n",
      "5e-05 & 0.2000 & 1e-04 & 186 & 0.4003 & 0.4908 & 0.8224 & 0.7464 \\\\\n",
      "5e-05 & 0.1000 & 0e+00 & 190 & 0.3810 & 0.4914 & 0.8375 & 0.7427 \\\\\n",
      "1e-05 & 0.2000 & 1e-04 & 748 & 0.4405 & 0.4916 & 0.7947 & 0.7536 \\\\\n",
      "5e-05 & 0.0000 & 5e-04 & 247 & 0.4263 & 0.4918 & 0.8102 & 0.7417 \\\\\n",
      "5e-05 & 0.1000 & 1e-04 & 170 & 0.4093 & 0.4918 & 0.8201 & 0.7438 \\\\\n",
      "1e-05 & 0.2000 & 5e-04 & 998 & 0.4446 & 0.4920 & 0.7952 & 0.7536 \\\\\n",
      "1e-05 & 0.0000 & 1e-04 & 785 & 0.4188 & 0.4920 & 0.8119 & 0.7490 \\\\\n",
      "1e-05 & 0.1000 & 5e-04 & 998 & 0.4453 & 0.4923 & 0.7950 & 0.7552 \\\\\n",
      "5e-05 & 0.0000 & 1e-04 & 173 & 0.4081 & 0.4925 & 0.8192 & 0.7448 \\\\\n",
      "5e-04 & 0.1000 & 1e-04 & 22 & 0.3747 & 0.4926 & 0.8352 & 0.7547 \\\\\n",
      "1e-04 & 0.1000 & 1.0e-03 & 176 & 0.4443 & 0.4926 & 0.7967 & 0.7474 \\\\\n",
      "1e-05 & 0.0000 & 0e+00 & 736 & 0.4252 & 0.4927 & 0.8085 & 0.7526 \\\\\n",
      "5e-04 & 0.2000 & 5e-04 & 40 & 0.3578 & 0.4931 & 0.8501 & 0.7427 \\\\\n",
      "5e-05 & 0.1000 & 1.0e-03 & 380 & 0.4456 & 0.4931 & 0.7996 & 0.7495 \\\\\n",
      "5e-04 & 0.1000 & 5e-04 & 27 & 0.4023 & 0.4932 & 0.8207 & 0.7406 \\\\\n",
      "5e-05 & 0.0000 & 0e+00 & 181 & 0.3866 & 0.4933 & 0.8343 & 0.7438 \\\\\n",
      "1e-04 & 0.1000 & 0e+00 & 79 & 0.4111 & 0.4934 & 0.8176 & 0.7490 \\\\\n",
      "5e-04 & 0.0000 & 1.0e-03 & 82 & 0.3774 & 0.4940 & 0.8448 & 0.7422 \\\\\n",
      "5e-04 & 0.0000 & 1e-04 & 23 & 0.3661 & 0.4942 & 0.8469 & 0.7464 \\\\\n",
      "1e-05 & 0.0000 & 5e-04 & 1000 & 0.4464 & 0.4943 & 0.7927 & 0.7500 \\\\\n",
      "1e-04 & 0.0000 & 1e-04 & 88 & 0.4147 & 0.4944 & 0.8167 & 0.7453 \\\\\n",
      "5e-04 & 0.0000 & 5e-04 & 36 & 0.3662 & 0.4949 & 0.8469 & 0.7474 \\\\\n",
      "1e-05 & 0.2000 & 0e+00 & 667 & 0.4422 & 0.4955 & 0.7939 & 0.7448 \\\\\n",
      "5e-04 & 0.1000 & 1.0e-03 & 36 & 0.4554 & 0.4960 & 0.7895 & 0.7432 \\\\\n",
      "5e-04 & 0.1000 & 0e+00 & 22 & 0.3490 & 0.4960 & 0.8544 & 0.7417 \\\\\n",
      "5e-04 & 0.2000 & 1e-04 & 27 & 0.3525 & 0.4965 & 0.8508 & 0.7391 \\\\\n",
      "5e-04 & 0.0000 & 0e+00 & 19 & 0.3677 & 0.5002 & 0.8440 & 0.7500 \\\\\n",
      "5e-05 & 0.0000 & 1.0e-03 & 237 & 0.4866 & 0.5017 & 0.7623 & 0.7469 \\\\\n",
      "5e-04 & 0.2000 & 0e+00 & 25 & 0.3460 & 0.5018 & 0.8527 & 0.7568 \\\\\n",
      "1e-05 & 0.2000 & 1.0e-03 & 997 & 0.4894 & 0.5021 & 0.7602 & 0.7458 \\\\\n",
      "1e-05 & 0.0000 & 1.0e-03 & 924 & 0.4931 & 0.5033 & 0.7564 & 0.7510 \\\\\n",
      "1e-05 & 0.1000 & 1.0e-03 & 850 & 0.4960 & 0.5036 & 0.7540 & 0.7495 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Leonard with cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "learning_rate & dropout_rate & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "5e-04 & 0.1000 & 1.0e-03 & 53 & 0.4726 & 0.5753 & 0.7854 & 0.6960 \\\\\n",
      "5e-04 & 0.0000 & 1.0e-03 & 48 & 0.4688 & 0.5778 & 0.7882 & 0.6927 \\\\\n",
      "1e-04 & 0.2000 & 5e-04 & 116 & 0.4655 & 0.5781 & 0.7883 & 0.7003 \\\\\n",
      "5e-04 & 0.2000 & 1.0e-03 & 43 & 0.4873 & 0.5784 & 0.7707 & 0.6979 \\\\\n",
      "5e-04 & 0.2000 & 5e-04 & 25 & 0.4735 & 0.5784 & 0.7819 & 0.6951 \\\\\n",
      "1e-04 & 0.1000 & 5e-04 & 102 & 0.4759 & 0.5786 & 0.7800 & 0.6993 \\\\\n",
      "1e-04 & 0.0000 & 1.0e-03 & 155 & 0.4939 & 0.5788 & 0.7679 & 0.6937 \\\\\n",
      "1e-04 & 0.1000 & 1e-04 & 63 & 0.4812 & 0.5788 & 0.7721 & 0.6979 \\\\\n",
      "5e-05 & 0.1000 & 5e-04 & 215 & 0.4660 & 0.5792 & 0.7863 & 0.6965 \\\\\n",
      "1e-04 & 0.0000 & 5e-04 & 113 & 0.4598 & 0.5795 & 0.7916 & 0.6974 \\\\\n",
      "1e-04 & 0.2000 & 1.0e-03 & 159 & 0.4988 & 0.5795 & 0.7626 & 0.6922 \\\\\n",
      "5e-05 & 0.2000 & 5e-04 & 213 & 0.4784 & 0.5799 & 0.7775 & 0.6951 \\\\\n",
      "5e-05 & 0.1000 & 1.0e-03 & 277 & 0.5093 & 0.5800 & 0.7528 & 0.6918 \\\\\n",
      "5e-05 & 0.1000 & 0e+00 & 130 & 0.4685 & 0.5808 & 0.7821 & 0.6965 \\\\\n",
      "1e-04 & 0.2000 & 0e+00 & 63 & 0.4811 & 0.5808 & 0.7736 & 0.6993 \\\\\n",
      "1e-04 & 0.1000 & 1.0e-03 & 154 & 0.4964 & 0.5810 & 0.7669 & 0.6875 \\\\\n",
      "5e-05 & 0.0000 & 0e+00 & 122 & 0.4705 & 0.5810 & 0.7782 & 0.6937 \\\\\n",
      "1e-05 & 0.0000 & 0e+00 & 578 & 0.4792 & 0.5810 & 0.7739 & 0.6908 \\\\\n",
      "1e-04 & 0.0000 & 0e+00 & 56 & 0.4828 & 0.5811 & 0.7691 & 0.6913 \\\\\n",
      "1e-04 & 0.2000 & 1e-04 & 71 & 0.4794 & 0.5811 & 0.7753 & 0.6955 \\\\\n",
      "1e-04 & 0.0000 & 1e-04 & 70 & 0.4668 & 0.5814 & 0.7849 & 0.6937 \\\\\n",
      "5e-05 & 0.0000 & 1e-04 & 131 & 0.4793 & 0.5814 & 0.7741 & 0.7022 \\\\\n",
      "5e-05 & 0.0000 & 5e-04 & 179 & 0.4865 & 0.5816 & 0.7703 & 0.6932 \\\\\n",
      "1e-05 & 0.2000 & 0e+00 & 575 & 0.4978 & 0.5816 & 0.7609 & 0.6922 \\\\\n",
      "5e-05 & 0.1000 & 1e-04 & 138 & 0.4726 & 0.5818 & 0.7823 & 0.6903 \\\\\n",
      "1e-05 & 0.2000 & 1e-04 & 499 & 0.5172 & 0.5818 & 0.7421 & 0.6828 \\\\\n",
      "5e-05 & 0.2000 & 1e-04 & 137 & 0.4870 & 0.5820 & 0.7692 & 0.6932 \\\\\n",
      "5e-05 & 0.2000 & 1.0e-03 & 251 & 0.5181 & 0.5824 & 0.7437 & 0.6955 \\\\\n",
      "5e-05 & 0.0000 & 1.0e-03 & 269 & 0.5100 & 0.5826 & 0.7512 & 0.6866 \\\\\n",
      "1e-04 & 0.1000 & 0e+00 & 72 & 0.4509 & 0.5838 & 0.7954 & 0.6922 \\\\\n",
      "5e-05 & 0.2000 & 0e+00 & 128 & 0.4854 & 0.5839 & 0.7671 & 0.6937 \\\\\n",
      "1e-05 & 0.0000 & 1e-04 & 443 & 0.5207 & 0.5841 & 0.7385 & 0.6875 \\\\\n",
      "5e-04 & 0.1000 & 5e-04 & 28 & 0.4435 & 0.5843 & 0.8045 & 0.6918 \\\\\n",
      "5e-04 & 0.0000 & 5e-04 & 29 & 0.4328 & 0.5852 & 0.8122 & 0.6970 \\\\\n",
      "1e-05 & 0.1000 & 0e+00 & 358 & 0.5311 & 0.5855 & 0.7316 & 0.6823 \\\\\n",
      "1e-05 & 0.2000 & 5e-04 & 585 & 0.5300 & 0.5857 & 0.7334 & 0.6837 \\\\\n",
      "1e-05 & 0.1000 & 1e-04 & 354 & 0.5389 & 0.5862 & 0.7237 & 0.6832 \\\\\n",
      "5e-04 & 0.0000 & 1e-04 & 17 & 0.4472 & 0.5883 & 0.7987 & 0.6932 \\\\\n",
      "1e-05 & 0.1000 & 5e-04 & 384 & 0.5494 & 0.5885 & 0.7160 & 0.6832 \\\\\n",
      "1e-05 & 0.0000 & 5e-04 & 308 & 0.5570 & 0.5911 & 0.7093 & 0.6832 \\\\\n",
      "5e-04 & 0.2000 & 0e+00 & 17 & 0.4532 & 0.5912 & 0.7919 & 0.6974 \\\\\n",
      "5e-04 & 0.1000 & 0e+00 & 18 & 0.4244 & 0.5930 & 0.8142 & 0.6880 \\\\\n",
      "1e-05 & 0.2000 & 1.0e-03 & 323 & 0.5673 & 0.5931 & 0.7039 & 0.6813 \\\\\n",
      "5e-04 & 0.2000 & 1e-04 & 21 & 0.4366 & 0.5937 & 0.8043 & 0.6880 \\\\\n",
      "1e-05 & 0.0000 & 1.0e-03 & 336 & 0.5658 & 0.5944 & 0.7040 & 0.6757 \\\\\n",
      "1e-05 & 0.1000 & 1.0e-03 & 242 & 0.5716 & 0.5947 & 0.6992 & 0.6804 \\\\\n",
      "5e-04 & 0.1000 & 1e-04 & 17 & 0.4540 & 0.5956 & 0.7936 & 0.6951 \\\\\n",
      "5e-04 & 0.0000 & 0e+00 & 16 & 0.4330 & 0.5991 & 0.8069 & 0.6818 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Leonard with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "learning_rate & dropout_rate & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "1e-04 & 0.2000 & 1.0e-03 & 183 & 0.4808 & 0.5773 & 0.7805 & 0.7012 \\\\\n",
      "5e-04 & 0.2000 & 1.0e-03 & 49 & 0.4801 & 0.5773 & 0.7809 & 0.7017 \\\\\n",
      "1e-04 & 0.2000 & 5e-04 & 101 & 0.4784 & 0.5782 & 0.7798 & 0.7008 \\\\\n",
      "5e-04 & 0.1000 & 1.0e-03 & 40 & 0.4866 & 0.5803 & 0.7778 & 0.6894 \\\\\n",
      "1e-04 & 0.1000 & 5e-04 & 97 & 0.4767 & 0.5813 & 0.7851 & 0.6913 \\\\\n",
      "1e-04 & 0.1000 & 1.0e-03 & 148 & 0.4985 & 0.5814 & 0.7672 & 0.6951 \\\\\n",
      "1e-04 & 0.0000 & 1.0e-03 & 182 & 0.4830 & 0.5814 & 0.7809 & 0.6984 \\\\\n",
      "5e-05 & 0.0000 & 1e-04 & 143 & 0.4542 & 0.5815 & 0.7987 & 0.7027 \\\\\n",
      "5e-05 & 0.2000 & 5e-04 & 204 & 0.4822 & 0.5820 & 0.7782 & 0.6951 \\\\\n",
      "5e-05 & 0.2000 & 1e-04 & 143 & 0.4713 & 0.5821 & 0.7854 & 0.6970 \\\\\n",
      "5e-05 & 0.0000 & 0e+00 & 115 & 0.4791 & 0.5822 & 0.7782 & 0.6922 \\\\\n",
      "5e-05 & 0.0000 & 5e-04 & 192 & 0.4778 & 0.5824 & 0.7818 & 0.6889 \\\\\n",
      "5e-05 & 0.1000 & 5e-04 & 194 & 0.4860 & 0.5824 & 0.7735 & 0.6899 \\\\\n",
      "5e-04 & 0.2000 & 5e-04 & 27 & 0.4615 & 0.5830 & 0.7899 & 0.6847 \\\\\n",
      "1e-04 & 0.0000 & 5e-04 & 110 & 0.4685 & 0.5830 & 0.7886 & 0.6965 \\\\\n",
      "5e-05 & 0.1000 & 1e-04 & 117 & 0.4942 & 0.5838 & 0.7649 & 0.6970 \\\\\n",
      "5e-04 & 0.1000 & 5e-04 & 27 & 0.4546 & 0.5839 & 0.7982 & 0.6951 \\\\\n",
      "5e-04 & 0.0000 & 5e-04 & 24 & 0.4673 & 0.5839 & 0.7901 & 0.7017 \\\\\n",
      "5e-05 & 0.2000 & 0e+00 & 120 & 0.4969 & 0.5840 & 0.7619 & 0.6932 \\\\\n",
      "5e-04 & 0.0000 & 1.0e-03 & 36 & 0.5004 & 0.5841 & 0.7635 & 0.6951 \\\\\n",
      "1e-05 & 0.0000 & 0e+00 & 503 & 0.5012 & 0.5843 & 0.7590 & 0.6903 \\\\\n",
      "1e-04 & 0.1000 & 0e+00 & 68 & 0.4662 & 0.5848 & 0.7850 & 0.6979 \\\\\n",
      "1e-05 & 0.2000 & 0e+00 & 510 & 0.5103 & 0.5854 & 0.7544 & 0.6884 \\\\\n",
      "1e-04 & 0.1000 & 1e-04 & 65 & 0.4839 & 0.5855 & 0.7737 & 0.6932 \\\\\n",
      "1e-05 & 0.1000 & 0e+00 & 466 & 0.5105 & 0.5855 & 0.7517 & 0.6927 \\\\\n",
      "1e-04 & 0.2000 & 0e+00 & 61 & 0.4895 & 0.5861 & 0.7706 & 0.6903 \\\\\n",
      "5e-05 & 0.1000 & 0e+00 & 117 & 0.4875 & 0.5861 & 0.7701 & 0.6889 \\\\\n",
      "1e-04 & 0.0000 & 0e+00 & 55 & 0.4880 & 0.5863 & 0.7691 & 0.6927 \\\\\n",
      "1e-04 & 0.0000 & 1e-04 & 68 & 0.4744 & 0.5863 & 0.7836 & 0.6932 \\\\\n",
      "1e-05 & 0.2000 & 1e-04 & 460 & 0.5243 & 0.5865 & 0.7397 & 0.6922 \\\\\n",
      "5e-05 & 0.0000 & 1.0e-03 & 241 & 0.5267 & 0.5866 & 0.7403 & 0.6880 \\\\\n",
      "1e-04 & 0.2000 & 1e-04 & 64 & 0.4918 & 0.5868 & 0.7667 & 0.7003 \\\\\n",
      "5e-05 & 0.2000 & 1.0e-03 & 183 & 0.5344 & 0.5874 & 0.7336 & 0.6870 \\\\\n",
      "1e-05 & 0.0000 & 1e-04 & 377 & 0.5292 & 0.5879 & 0.7352 & 0.6927 \\\\\n",
      "1e-05 & 0.2000 & 5e-04 & 526 & 0.5391 & 0.5883 & 0.7253 & 0.6903 \\\\\n",
      "5e-05 & 0.1000 & 1.0e-03 & 175 & 0.5389 & 0.5890 & 0.7286 & 0.6894 \\\\\n",
      "5e-04 & 0.2000 & 1e-04 & 21 & 0.4358 & 0.5895 & 0.8057 & 0.7055 \\\\\n",
      "1e-05 & 0.0000 & 5e-04 & 450 & 0.5424 & 0.5896 & 0.7226 & 0.6913 \\\\\n",
      "1e-05 & 0.1000 & 1e-04 & 336 & 0.5455 & 0.5906 & 0.7212 & 0.6884 \\\\\n",
      "1e-05 & 0.1000 & 5e-04 & 404 & 0.5533 & 0.5919 & 0.7137 & 0.6861 \\\\\n",
      "5e-04 & 0.2000 & 0e+00 & 17 & 0.4465 & 0.5931 & 0.7981 & 0.7045 \\\\\n",
      "5e-04 & 0.0000 & 1e-04 & 18 & 0.4423 & 0.5946 & 0.8046 & 0.6856 \\\\\n",
      "1e-05 & 0.2000 & 1.0e-03 & 335 & 0.5680 & 0.5953 & 0.7041 & 0.6823 \\\\\n",
      "1e-05 & 0.0000 & 1.0e-03 & 336 & 0.5670 & 0.5964 & 0.7039 & 0.6818 \\\\\n",
      "5e-04 & 0.1000 & 1e-04 & 20 & 0.4284 & 0.5970 & 0.8118 & 0.6861 \\\\\n",
      "5e-04 & 0.1000 & 0e+00 & 19 & 0.4132 & 0.5978 & 0.8210 & 0.6903 \\\\\n",
      "1e-05 & 0.1000 & 1.0e-03 & 209 & 0.5763 & 0.5985 & 0.6991 & 0.6809 \\\\\n",
      "5e-04 & 0.0000 & 0e+00 & 17 & 0.4243 & 0.6008 & 0.8133 & 0.6979 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [ \"all\", \"Sheldon_Penny\", \"Sheldon_Leonard\"]\n",
    "losses = [\"cross_entropy\", \"weighted_cross_entropy\"]\n",
    "model_type = \"classifier_only\" # one of 'classifier_only', 'lora', 'last_layer'\n",
    "\n",
    "for task, loss in itertools.product(tasks, losses):\n",
    "    log_dir = f\"../metrics/{model_type}/{task}/{loss}\"\n",
    "    print(f\"Results for {task} with {loss}:\")\n",
    "    print_results_latex(log_dir=log_dir, patience=3, max_epochs=20, model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a6dc1",
   "metadata": {},
   "source": [
    "Print latex tables for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035c21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all with cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "rank & alpha & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "32 & 64 & 1e-04 & 10 & 1.5629 & 1.6189 & 0.4061 & 0.3821 \\\\\n",
      "8 & 16 & 0e+00 & 16 & 1.5650 & 1.6243 & 0.4038 & 0.3750 \\\\\n",
      "16 & 32 & 0e+00 & 13 & 1.5538 & 1.6273 & 0.4082 & 0.3750 \\\\\n",
      "16 & 32 & 1e-04 & 10 & 1.5901 & 1.6277 & 0.3940 & 0.3741 \\\\\n",
      "8 & 16 & 1e-04 & 12 & 1.5944 & 1.6303 & 0.3946 & 0.3699 \\\\\n",
      "32 & 64 & 0e+00 & 7 & 1.5943 & 1.6333 & 0.3954 & 0.3670 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for all with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "rank & alpha & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "16 & 32 & 0e+00 & 11 & 1.6833 & 1.7406 & 0.3443 & 0.3290 \\\\\n",
      "8 & 16 & 1e-04 & 10 & 1.7172 & 1.7422 & 0.3323 & 0.3135 \\\\\n",
      "32 & 64 & 1e-04 & 9 & 1.6836 & 1.7438 & 0.3438 & 0.3152 \\\\\n",
      "8 & 16 & 0e+00 & 10 & 1.7178 & 1.7465 & 0.3339 & 0.3026 \\\\\n",
      "16 & 32 & 1e-04 & 11 & 1.6899 & 1.7508 & 0.3462 & 0.3192 \\\\\n",
      "32 & 64 & 0e+00 & 6 & 1.7189 & 1.7530 & 0.3375 & 0.3138 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Penny with cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "rank & alpha & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "32 & 64 & 1e-04 & 6 & 0.4285 & 0.4520 & 0.7929 & 0.7859 \\\\\n",
      "8 & 16 & 1e-04 & 9 & 0.4374 & 0.4530 & 0.7886 & 0.7833 \\\\\n",
      "8 & 16 & 0e+00 & 12 & 0.4192 & 0.4534 & 0.7951 & 0.7818 \\\\\n",
      "16 & 32 & 0e+00 & 8 & 0.4264 & 0.4545 & 0.7917 & 0.7828 \\\\\n",
      "16 & 32 & 1e-04 & 9 & 0.4224 & 0.4567 & 0.7970 & 0.7781 \\\\\n",
      "32 & 64 & 0e+00 & 6 & 0.4272 & 0.4701 & 0.7944 & 0.7667 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Penny with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "rank & alpha & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "16 & 32 & 0e+00 & 10 & 0.4233 & 0.4604 & 0.7905 & 0.7745 \\\\\n",
      "16 & 32 & 1e-04 & 9 & 0.4311 & 0.4664 & 0.7882 & 0.7818 \\\\\n",
      "8 & 16 & 0e+00 & 12 & 0.4253 & 0.4698 & 0.7887 & 0.7542 \\\\\n",
      "32 & 64 & 1e-04 & 6 & 0.4430 & 0.4726 & 0.7784 & 0.7385 \\\\\n",
      "8 & 16 & 1e-04 & 11 & 0.4373 & 0.4758 & 0.7840 & 0.7562 \\\\\n",
      "32 & 64 & 0e+00 & 8 & 0.4147 & 0.4769 & 0.7973 & 0.7625 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Leonard with cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "rank & alpha & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "8 & 16 & 1e-04 & 13 & 0.4929 & 0.5291 & 0.7512 & 0.7277 \\\\\n",
      "16 & 32 & 1e-04 & 11 & 0.4852 & 0.5320 & 0.7577 & 0.7249 \\\\\n",
      "32 & 64 & 0e+00 & 8 & 0.4843 & 0.5326 & 0.7562 & 0.7211 \\\\\n",
      "8 & 16 & 0e+00 & 10 & 0.5027 & 0.5337 & 0.7450 & 0.7183 \\\\\n",
      "32 & 64 & 1e-04 & 6 & 0.5074 & 0.5346 & 0.7444 & 0.7225 \\\\\n",
      "16 & 32 & 0e+00 & 13 & 0.4633 & 0.5421 & 0.7684 & 0.7301 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Leonard with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "rank & alpha & weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "16 & 32 & 0e+00 & 10 & 0.4905 & 0.5284 & 0.7500 & 0.7292 \\\\\n",
      "8 & 16 & 1e-04 & 12 & 0.4983 & 0.5286 & 0.7437 & 0.7367 \\\\\n",
      "16 & 32 & 1e-04 & 11 & 0.4880 & 0.5343 & 0.7558 & 0.7159 \\\\\n",
      "8 & 16 & 0e+00 & 6 & 0.5310 & 0.5365 & 0.7231 & 0.7268 \\\\\n",
      "32 & 64 & 1e-04 & 6 & 0.5112 & 0.5378 & 0.7364 & 0.7259 \\\\\n",
      "32 & 64 & 0e+00 & 7 & 0.4910 & 0.5430 & 0.7543 & 0.7202 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [ \"all\", \"Sheldon_Penny\", \"Sheldon_Leonard\"]\n",
    "losses = [\"cross_entropy\", \"weighted_cross_entropy\"]\n",
    "model_type = \"lora\"\n",
    "\n",
    "for task, loss in itertools.product(tasks, losses):\n",
    "    log_dir = f\"../metrics/{model_type}/{task}/{loss}\"\n",
    "    print(f\"Results for {task} with {loss}:\")\n",
    "    print_results_latex(log_dir=log_dir, patience=3, max_epochs=20, model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca06af6",
   "metadata": {},
   "source": [
    "Print latex tables for Last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7632a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all with cross_entropy:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "1e-04 & 6 & 1.5757 & 1.6287 & 0.3968 & 0.3726 \\\\\n",
      "0e+00 & 4 & 1.6036 & 1.6447 & 0.3863 & 0.3706 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for all with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "1e-04 & 6 & 1.6771 & 1.7650 & 0.3484 & 0.3210 \\\\\n",
      "0e+00 & 4 & 1.7177 & 1.7665 & 0.3298 & 0.3249 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Penny with cross_entropy:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "0e+00 & 7 & 0.4089 & 0.4642 & 0.7986 & 0.7745 \\\\\n",
      "1e-04 & 5 & 0.4437 & 0.4761 & 0.7799 & 0.7521 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Penny with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "1e-04 & 7 & 0.4319 & 0.4736 & 0.7839 & 0.7615 \\\\\n",
      "0e+00 & 6 & 0.4293 & 0.5134 & 0.7876 & 0.6953 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Leonard with cross_entropy:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "0e+00 & 5 & 0.5093 & 0.5433 & 0.7408 & 0.7154 \\\\\n",
      "1e-04 & 4 & 0.5325 & 0.5590 & 0.7220 & 0.7050 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Results for Sheldon_Leonard with weighted_cross_entropy:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "weight_decay & epoch & train_loss & val_loss & train_acc & val_acc \\\\\n",
      "\\midrule\n",
      "1e-04 & 4 & 0.5329 & 0.5507 & 0.7216 & 0.7135 \\\\\n",
      "0e+00 & 7 & 0.4801 & 0.5799 & 0.7603 & 0.6932 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [ \"all\", \"Sheldon_Penny\", \"Sheldon_Leonard\"]\n",
    "losses = [\"cross_entropy\", \"weighted_cross_entropy\"]\n",
    "model_type = \"last_layer\"\n",
    "\n",
    "for task, loss in itertools.product(tasks, losses):\n",
    "    log_dir = f\"../metrics/{model_type}/{task}/{loss}\"\n",
    "    print(f\"Results for {task} with {loss}:\")\n",
    "    print_results_latex(log_dir=log_dir, patience=3, max_epochs=20, model_type=model_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
