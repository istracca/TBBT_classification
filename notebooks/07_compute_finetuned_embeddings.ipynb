{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2ad591",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Adding 'src' directory to the system path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "from my_classes import SBERTWithClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa0225",
   "metadata": {},
   "source": [
    "### Load raw csv and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebf27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/1_10_seasons_tbbt.csv\", delimiter=',', encoding='utf8') \n",
    "df.rename(columns = {'person_scene':'Person', 'dialogue':'Said'}, inplace = True)\n",
    "df['Season'] = df['episode_name'].str.extract(r'Series (\\d{2})').astype(int)\n",
    "df = df[['Person', 'Said', 'Season']]\n",
    "\n",
    "# Remove \"Scene\" and \"(off)\"\n",
    "df = df[~df['Person'].isin(['Scene', '(off)'])]\n",
    "\n",
    "# Replace \"Cooper\" with \"Mary\"\n",
    "df['Person'] = df['Person'].replace({'Cooper': 'Mary'})\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940e398",
   "metadata": {},
   "source": [
    "### Keep only main characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = ['Sheldon', 'Leonard', 'Raj', 'Penny','Howard','Amy','Bernadette']\n",
    "data = df[df.Person.isin(persons)]\n",
    "print(len(data), \"dialogues for main characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a429a",
   "metadata": {},
   "source": [
    "### Drop empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b60112",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "print(len(data), \"dialogues for main characters after dropping empty lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3299a",
   "metadata": {},
   "source": [
    "### Load best model (Last-Layer with weighted_cross_entropy and wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3654b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "model = SBERTWithClassifier(base_model, num_classes=7, dropout_rate=0.1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"../models/last_layer/all/weighted_cross_entropy/last_layer_0.0001_0.0001.pt\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce40950",
   "metadata": {},
   "source": [
    "### Compute new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f421d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "embeddings_list = []\n",
    "\n",
    "# Extract embeddings in batches\n",
    "for i in tqdm(range(0, len(data), 32)):\n",
    "    batch_texts = data[\"Said\"].iloc[i:i+32].tolist()\n",
    "    encoded = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.sbert(**encoded)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token embeddings\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "# Build a new DataFrame with the embeddings\n",
    "new_df = pd.DataFrame({\n",
    "    \"Person\": data[\"Person\"].values,\n",
    "    \"Said\": data[\"Said\"].values,\n",
    "    \"Season\": data[\"Season\"].values,\n",
    "    \"Embedding\": list(embeddings)\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a pickle file\n",
    "new_df.to_pickle(\"../data/processed/sfinetuned_embeddings.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
