{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de28499",
   "metadata": {},
   "source": [
    "### Import libraries and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Adding 'src' directory to the system path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "from my_classes import EmbeddingDataset, EmbeddingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2488d",
   "metadata": {},
   "source": [
    "### Set the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"all\" # has to be one of 'all', 'Sheldon_Penny', 'Sheldon_Leonard'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a97236",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db66409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from the pickle file\n",
    "df = pd.read_pickle(\"../data/processed/sbert_mini_embeddings.pkl\")\n",
    "if task == \"Sheldon_Leonard\":\n",
    "    df =df[df['Person'].isin(['Sheldon','Leonard'])]\n",
    "elif task == \"Sheldon_Penny\":\n",
    "    df = df[df['Person'].isin(['Sheldon','Penny'])]\n",
    "elif task != \"all\":\n",
    "    print(\"Task not recognized, using all data.\")\n",
    "\n",
    "# Convert embeddings to a tensor\n",
    "X = np.stack(df[\"Embedding\"].values)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Label encoding for the 'Person' column\n",
    "label_encoder = LabelEncoder()\n",
    "y = torch.tensor(label_encoder.fit_transform(df[\"Person\"]), dtype=torch.long)\n",
    "y_np = y.numpy()\n",
    "all_classes = np.unique(y_np)\n",
    "# print(len(all_classes))\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "train_dataset = EmbeddingDataset(X_train, y_train)\n",
    "val_dataset = EmbeddingDataset(X_val, y_val)\n",
    "\n",
    "# create loaders for training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3891f",
   "metadata": {},
   "source": [
    "### Set seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True  # disable non-deterministic optimizations\n",
    "torch.backends.cudnn.benchmark = False     # disable benchmarking for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a318cd0",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters for the grid search\n",
    "learning_rates = [1e-5, 5e-5, 1e-4, 5e-4]\n",
    "dropout_rates = [0, 0.1, 0.2]\n",
    "weight_decays = [0, 1e-4, 5e-4, 1e-3]\n",
    "losses = [\"cross_entropy\",\"weighted_cross_entropy\"]\n",
    "\n",
    "for lr, dropout_rate, weight_decay, loss_name in itertools.product(learning_rates, dropout_rates, weight_decays, losses):\n",
    "\n",
    "  model = EmbeddingClassifier(input_dim=384, num_classes=len(all_classes), dropout_rate=dropout_rate).to(device)\n",
    "  optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "  # Define the loss function\n",
    "  if loss_name == \"weighted_cross_entropy\":\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=all_classes, y=y_np)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "  elif loss_name == \"cross_entropy\":\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Function to compute accuracy\n",
    "  def compute_accuracy(logits, labels):\n",
    "      preds = torch.argmax(logits, dim=1)\n",
    "      return (preds == labels).float().mean().item()\n",
    "\n",
    "  # To monitor training and validation losses and accuracies\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  train_accuracies = []\n",
    "  val_accuracies = []\n",
    "  best_val_loss = 10.0    # An arbitrary high value for initial best validation loss\n",
    "  best_model_path = f\"../models/classifier_only/{task}/{loss_name}/best_classifier_{lr}_{dropout_rate}_{weight_decay}.pt\"\n",
    "  # print(best_model_path)\n",
    "\n",
    "  # Early stopping settings\n",
    "  patience = 10\n",
    "  no_improvement = 0\n",
    "  num_epochs = 1000\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        inputs = batch[\"embedding\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_acc += compute_accuracy(outputs, labels)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_train_acc = total_train_acc / len(train_loader)\n",
    "\n",
    "    # ---- VAL ----\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "            inputs = batch[\"embedding\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            total_val_acc += compute_accuracy(outputs, labels)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_val_acc = total_val_acc / len(val_loader)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "\n",
    "    # ---- Early Stopping ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"üíæ Best model saved (val loss: {best_val_loss:.4f})\")\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement for {no_improvement} epoch(s)\")\n",
    "\n",
    "    # ---- Print Summary for the epoch ----\n",
    "    print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  üîπ Train Loss: {avg_train_loss:.4f} | Accuracy: {avg_train_acc:.4f}\")\n",
    "    print(f\"  üî∏ Val   Loss: {avg_val_loss:.4f} | Accuracy: {avg_val_acc:.4f}\\n\")\n",
    "\n",
    "    # ---- Early Stopping Check ----\n",
    "    if no_improvement >= patience:\n",
    "        print(f\"‚èπÔ∏è Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "    \n",
    "  # ---- Save Metrics ----\n",
    "  metrics_df = pd.DataFrame({\n",
    "    \"epoch\": list(range(1, len(train_losses) + 1)),\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_loss\": val_losses,\n",
    "    \"train_accuracy\": train_accuracies,\n",
    "    \"val_accuracy\": val_accuracies,\n",
    "    \"learning_rate\": lr,\n",
    "    \"dropout_rate\": dropout_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "  })\n",
    "\n",
    "  # Save the metrics DataFrame to a CSV file\n",
    "  csv_path = f\"../metrics/classifier_only/{task}/{loss_name}/metrics_{lr}_{dropout_rate}_{weight_decay}.csv\"\n",
    "  metrics_df.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
